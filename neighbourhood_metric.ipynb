{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings file from ../data/embeddings/crawl-300d-2M-subword.vec\n",
      "2000000\n",
      "2000000\n",
      "Saving binary file to ../data/embeddings/crawl-300d-2M-subword.vec.wv\n",
      "Saving vocabulary file to ../data/embeddings/crawl-300d-2M-subword.vec.vocab\n"
     ]
    }
   ],
   "source": [
    "%run save_embeds_2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "def load_embeddings_from_np(filename):\n",
    "    print('loading ...')\n",
    "    with codecs.open(filename + '.vocab', 'r', 'utf-8') as f_embed:\n",
    "        vocab = [line.strip() for line in f_embed]\n",
    "        \n",
    "    w2i = {w: i for i, w in enumerate(vocab)}\n",
    "    wv = np.load(filename + '.wv.npy')\n",
    "\n",
    "    return vocab, wv, w2i\n",
    "\n",
    "\n",
    "def normalize(wv):\n",
    "    \n",
    "    # normalize vectors\n",
    "    norms = np.apply_along_axis(LA.norm, 1, wv)\n",
    "    wv = wv / norms[:, np.newaxis]\n",
    "    return wv\n",
    "\n",
    "\n",
    "def load_and_normalize(space, filename, vocab, wv, w2i):\n",
    "    vocab_muse, wv_muse, w2i_muse = load_embeddings_from_np(filename)\n",
    "    wv_muse = normalize(wv_muse)\n",
    "    vocab[space] = vocab_muse \n",
    "    wv[space] = wv_muse\n",
    "    w2i[space] = w2i_muse\n",
    "    print('done')\n",
    "    \n",
    "\n",
    "def load_wo_normalize(space, filename, vocab, wv, w2i):\n",
    "    vocab_muse, wv_muse, w2i_muse = load_embeddings_from_np(filename)\n",
    "    vocab[space] = vocab_muse \n",
    "    wv[space] = wv_muse\n",
    "    w2i[space] = w2i_muse\n",
    "    print('done')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n",
      "loading ...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "wv = {}\n",
    "w2i = {}\n",
    "\n",
    "load_and_normalize('glove_1', '../data/embeddings/glove.twitter.27B.25d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_2', '../data/embeddings/glove.twitter.27B.50d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_3', '../data/embeddings/glove.twitter.27B.100d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_4', '../data/embeddings/glove.twitter.27B.200d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_5', '../data/embeddings/glove.6B.50d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_6', '../data/embeddings/glove.6B.100d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_7', '../data/embeddings/glove.6B.200d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_8', '../data/embeddings/glove.6B.300d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_9', '../data/embeddings/glove.42B.300d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('glove_10', '../data/embeddings/glove.840B.300d.txt', vocab, wv, w2i)\n",
    "load_and_normalize('fasttext_1', '../data/embeddings/wiki-news-300d-1M.vec', vocab, wv, w2i)\n",
    "load_and_normalize('fasttext_2', '../data/embeddings/wiki-news-300d-1M-subword.vec', vocab, wv, w2i)\n",
    "load_and_normalize('fasttext_3', '../data/embeddings/crawl-300d-2M.vec', vocab, wv, w2i)\n",
    "load_and_normalize('fasttext_4', '../data/embeddings/crawl-300d-2M-subword.vec', vocab, wv, w2i)\n",
    "load_and_normalize('w2v', '../data/embeddings/orig_w2v', vocab, wv, w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def topK(w, space, k=10):\n",
    "    \n",
    "    # extract the word vector for word w\n",
    "    idx = w2i[space][w]\n",
    "    vec = wv[space][idx, :]\n",
    "    \n",
    "    # compute similarity of w with all words in the vocabulary\n",
    "    sim = wv[space].dot(vec)\n",
    "    # sort similarities by descending order\n",
    "    sort_sim = (sim.argsort())[::-1]\n",
    "\n",
    "    # choose topK\n",
    "    best = sort_sim[:(k+1)]\n",
    "\n",
    "    return [vocab[space][i] for i in best if i!=idx]\n",
    "\n",
    "\n",
    "def similarity(w1, w2, space):\n",
    "    \n",
    "    i1 = w2i[space][w1]\n",
    "    i2 = w2i[space][w2]\n",
    "    vec1 = wv[space][i1, :]\n",
    "    vec2 = wv[space][i2, :]\n",
    "\n",
    "    return np.inner(vec1,vec2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restrict vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "\n",
    "def has_punct(w):\n",
    "    \n",
    "    if any([c in string.punctuation for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_digit(w):\n",
    "    \n",
    "    if any([c in '0123456789' for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def limit_vocab(space, exclude = None):\n",
    "    vocab_limited = []\n",
    "    for w in tqdm(vocab[space][:50000]): \n",
    "        if w.lower() != w:\n",
    "            continue\n",
    "        if len(w) >= 20:\n",
    "            continue\n",
    "        if has_digit(w):\n",
    "            continue\n",
    "        if '_' in w:\n",
    "            p = [has_punct(subw) for subw in w.split('_')]\n",
    "            if not any(p):\n",
    "                vocab_limited.append(w)\n",
    "            continue\n",
    "        if has_punct(w):\n",
    "            continue\n",
    "        vocab_limited.append(w)\n",
    "    \n",
    "    if exclude:\n",
    "        vocab_limited = list(set(vocab_limited) - set(exclude))\n",
    "    \n",
    "    print(\"size of vocabulary:\", len(vocab_limited))\n",
    "    \n",
    "    wv_limited = np.zeros((len(vocab_limited), 300))\n",
    "    for i,w in enumerate(vocab_limited):\n",
    "        wv_limited[i,:] = wv[space][w2i[space][w],:]\n",
    "    \n",
    "    w2i_limited = {w: i for i, w in enumerate(vocab_limited)}\n",
    "    \n",
    "    return vocab_limited, wv_limited, w2i_limited\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "\n",
    "\n",
    "def has_punct(w):\n",
    "    \n",
    "    if any([c in string.punctuation for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def has_digit(w):\n",
    "    \n",
    "    if any([c in '0123456789' for c in w]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def limit_vocab(space, exclude = None, vec_len):\n",
    "    vocab_limited = []\n",
    "    for w in tqdm(vocab[space][:50000]): \n",
    "        if w.lower() != w:\n",
    "            continue\n",
    "        if len(w) >= 20:\n",
    "            continue\n",
    "        if has_digit(w):\n",
    "            continue\n",
    "        if '_' in w:\n",
    "            p = [has_punct(subw) for subw in w.split('_')]\n",
    "            if not any(p):\n",
    "                vocab_limited.append(w)\n",
    "            continue\n",
    "        if has_punct(w):\n",
    "            continue\n",
    "        vocab_limited.append(w)\n",
    "    \n",
    "    if exclude:\n",
    "        vocab_limited = list(set(vocab_limited) - set(exclude))\n",
    "    \n",
    "    print(\"size of vocabulary:\", len(vocab_limited))\n",
    "    \n",
    "    wv_limited = np.zeros((len(vocab_limited), vec_len))\n",
    "    for i,w in enumerate(vocab_limited):\n",
    "        wv_limited[i,:] = wv[space][w2i[space][w],:]\n",
    "    \n",
    "    w2i_limited = {w: i for i, w in enumerate(vocab_limited)}\n",
    "    \n",
    "    return vocab_limited, wv_limited, w2i_limited\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 423061.96it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 444855.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 49288\n",
      "size of vocabulary: 49288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 438508.13it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 441419.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 49288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 49288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 421935.01it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 431659.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 45530\n",
      "size of vocabulary: 45530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 425701.84it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 431272.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 45530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 45530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 434805.11it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 681608.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 42716\n",
      "size of vocabulary: 23097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 609814.48it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 575448.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 26800\n",
      "size of vocabulary: 26800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 576356.00it/s]\n",
      "100%|██████████| 50000/50000 [00:00<00:00, 589102.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 27174\n",
      "size of vocabulary: 27231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:00<00:00, 598386.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocabulary: 26189\n"
     ]
    }
   ],
   "source": [
    "# create the reduced vocabularies and embeddings before and after, without gendered specific words\n",
    "\n",
    "import json\n",
    "with codecs.open('../data/lists/gender_specific_full.json') as f:\n",
    "    gender_specific = json.load(f)\n",
    "with codecs.open('../data/lists/definitional_pairs.json') as f:\n",
    "    definitional_pairs = json.load(f)\n",
    "with codecs.open('../data/lists/equalize_pairs.json') as f:\n",
    "    equalize_pairs = json.load(f)\n",
    "\n",
    "exclude_words = []\n",
    "for pair in definitional_pairs + equalize_pairs:\n",
    "    exclude_words.append(pair[0])\n",
    "    exclude_words.append(pair[1])\n",
    "\n",
    "exclude_words = list(set(exclude_words).union(set(gender_specific)))\n",
    "\n",
    "# create spaces of limited vocabulary\n",
    "vocab['limit_glove_1'], wv['limit_glove_1'], w2i['limit_glove_1'] = limit_vocab('glove_1', exclude = exclude_words, vec_len=25)\n",
    "vocab['limit_glove_2'], wv['limit_glove_2'], w2i['limit_glove_2'] = limit_vocab('glove_2', exclude = exclude_words, vec_len=50)\n",
    "vocab['limit_glove_3'], wv['limit_glove_3'], w2i['limit_glove_3'] = limit_vocab('glove_3', exclude = exclude_words, vec_len=100)\n",
    "vocab['limit_glove_4'], wv['limit_glove_4'], w2i['limit_glove_4'] = limit_vocab('glove_4', exclude = exclude_words, vec_len=200)\n",
    "vocab['limit_glove_5'], wv['limit_glove_5'], w2i['limit_glove_5'] = limit_vocab('glove_5', exclude = exclude_words, vec_len=50)\n",
    "vocab['limit_glove_6'], wv['limit_glove_6'], w2i['limit_glove_6'] = limit_vocab('glove_6', exclude = exclude_words, vec_len=100)\n",
    "vocab['limit_glove_7'], wv['limit_glove_7'], w2i['limit_glove_7'] = limit_vocab('glove_7', exclude = exclude_words, vec_len=200)\n",
    "vocab['limit_glove_8'], wv['limit_glove_8'], w2i['limit_glove_8'] = limit_vocab('glove_8', exclude = exclude_words, vec_len=300)\n",
    "vocab['limit_glove_9'], wv['limit_glove_9'], w2i['limit_glove_9'] = limit_vocab('glove_9', exclude = exclude_words, vec_len=300)\n",
    "vocab['limit_glove_10'], wv['limit_glove_10'], w2i['limit_glove_10'] = limit_vocab('glove_10', exclude = exclude_words, vec_len=300)\n",
    "\n",
    "vocab['limit_fasttext_1'], wv['limit_fasttext_1'], w2i['limit_fasttext_1'] = limit_vocab('fasttext_1', exclude = exclude_words, vec_len=300)\n",
    "vocab['limit_fasttext_2'], wv['limit_fasttext_2'], w2i['limit_fasttext_2'] = limit_vocab('fasttext_2', exclude = exclude_words, vec_len=300)\n",
    "vocab['limit_fasttext_3'], wv['limit_fasttext_3'], w2i['limit_fasttext_3'] = limit_vocab('fasttext_3', exclude = exclude_words, vec_len=300)\n",
    "vocab['limit_fasttext_4'], wv['limit_fasttext_4'], w2i['limit_fasttext_4'] = limit_vocab('fasttext_4', exclude = exclude_words, vec_len=300)\n",
    "\n",
    "vocab['limit_w2v'], wv['limit_w2v'], w2i['limit_w2v'] = limit_vocab('w2v', exclude = exclude_words, vec_len=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute bias-by-projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of the bias, before and after\n",
    "\n",
    "def compute_bias_by_projection(space_to_tag, full_space):\n",
    "    males = wv[space_to_tag].dot(wv[full_space][w2i[full_space]['he'],:])\n",
    "    females = wv[space_to_tag].dot(wv[full_space][w2i[full_space]['she'],:])\n",
    "    d = {}\n",
    "    for w,m,f in zip(vocab[space_to_tag], males, females):\n",
    "        d[w] = m-f\n",
    "    return d\n",
    "\n",
    "# compute bias-by-projection before and after debiasing\n",
    "gender_bias_glove_1 = compute_bias_by_projection('limit_glove_1', 'glove_1')\n",
    "gender_bias_glove_2 = compute_bias_by_projection('limit_glove_2', 'glove_2')\n",
    "gender_bias_glove_3 = compute_bias_by_projection('limit_glove_3', 'glove_3')\n",
    "gender_bias_glove_4 = compute_bias_by_projection('limit_glove_4', 'glove_4')\n",
    "gender_bias_glove_5 = compute_bias_by_projection('limit_glove_5', 'glove_5')\n",
    "gender_bias_glove_6 = compute_bias_by_projection('limit_glove_6', 'glove_6')\n",
    "gender_bias_glove_7 = compute_bias_by_projection('limit_glove_7', 'glove_7')\n",
    "gender_bias_glove_8 = compute_bias_by_projection('limit_glove_8', 'glove_8')\n",
    "gender_bias_glove_9 = compute_bias_by_projection('limit_glove_9', 'glove_9')\n",
    "gender_bias_glove_10 = compute_bias_by_projection('limit_glove_10', 'glove_10')\n",
    "\n",
    "gender_bias_fasttext_1 = compute_bias_by_projection('limit_fasttext_1', 'fasttext_1')\n",
    "gender_bias_fasttext_2 = compute_bias_by_projection('limit_fasttext_2', 'fasttext_2')\n",
    "gender_bias_fasttext_3 = compute_bias_by_projection('limit_fasttext_3', 'fasttext_3')\n",
    "gender_bias_fasttext_4 = compute_bias_by_projection('limit_fasttext_4', 'fasttext_4')\n",
    "\n",
    "gender_bias_w2v = compute_bias_by_projection('limit_w2v', 'w2v')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the avg bias of the vocabulary (abs) before and after debiasing\n",
    "\n",
    "def report_bias(gender_bias):\n",
    "    bias = 0.0\n",
    "    for k in gender_bias:\n",
    "        bias += np.abs(gender_bias[k])\n",
    "    print(bias/len(gender_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09540471479449965\n",
      "0.08798050874134192\n",
      "0.07545063847129732\n",
      "0.06236838290796072\n"
     ]
    }
   ],
   "source": [
    "report_bias(gender_bias_glove_1)\n",
    "report_bias(gender_bias_glove_2)\n",
    "report_bias(gender_bias_glove_3)\n",
    "report_bias(gender_bias_glove_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe Wiki Gigaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06009518969994563\n",
      "0.04789622113818426\n",
      "0.04561136886650126\n",
      "0.04346563770051508\n"
     ]
    }
   ],
   "source": [
    "report_bias(gender_bias_glove_5)\n",
    "report_bias(gender_bias_glove_6)\n",
    "report_bias(gender_bias_glove_7)\n",
    "report_bias(gender_bias_glove_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030484643981032403\n",
      "0.03909768263521343\n"
     ]
    }
   ],
   "source": [
    "report_bias(gender_bias_glove_9)\n",
    "report_bias(gender_bias_glove_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0460773100635275\n"
     ]
    }
   ],
   "source": [
    "report_bias(gender_bias_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fastText Wiki News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026961478262307038\n",
      "0.02273574827560815\n"
     ]
    }
   ],
   "source": [
    "report_bias(gender_bias_fasttext_1)\n",
    "report_bias(gender_bias_fasttext_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fastText Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03212379572532572\n",
      "0.02273328362188915\n"
     ]
    }
   ],
   "source": [
    "report_bias(gender_bias_fasttext_3)\n",
    "report_bias(gender_bias_fasttext_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "### Correlation between bias-by-projection and bias-by-neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tuples of biases and counts of masculine/feminine NN for each word (for bias-by-neighbors)\n",
    "\n",
    "def bias_by_neighbors(gender_bias, space, neighbours_num = 100):\n",
    "    \n",
    "    tuples = []\n",
    "    for w in tqdm(vocab[space]):\n",
    "        \n",
    "        top = topK(w, space, k=neighbours_num+5)[:neighbours_num]\n",
    "\n",
    "        m = 0\n",
    "        f = 0    \n",
    "        for t in top:\n",
    "            if gender_bias[t] > 0:\n",
    "                m+=1\n",
    "            else:\n",
    "                f+=1\n",
    "\n",
    "        tuples.append((w, gender_bias[w], m, f))\n",
    "\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49288/49288 [03:26<00:00, 238.74it/s]\n",
      "100%|██████████| 49288/49288 [03:32<00:00, 231.69it/s]\n",
      "100%|██████████| 49288/49288 [04:06<00:00, 200.18it/s]\n",
      "100%|██████████| 49288/49288 [04:50<00:00, 169.78it/s]\n",
      "100%|██████████| 45530/45530 [02:58<00:00, 254.42it/s]\n",
      "100%|██████████| 45530/45530 [03:27<00:00, 219.94it/s]\n",
      "100%|██████████| 45530/45530 [04:06<00:00, 184.64it/s]\n",
      "100%|██████████| 45530/45530 [04:41<00:00, 161.76it/s]\n",
      "100%|██████████| 42716/42716 [04:06<00:00, 172.99it/s]\n",
      "100%|██████████| 23097/23097 [01:10<00:00, 326.08it/s]\n",
      "100%|██████████| 26189/26189 [01:31<00:00, 286.57it/s]\n",
      "100%|██████████| 26800/26800 [01:35<00:00, 279.76it/s]\n",
      "100%|██████████| 26800/26800 [01:35<00:00, 280.02it/s]\n",
      "100%|██████████| 27174/27174 [01:38<00:00, 275.76it/s]\n",
      "100%|██████████| 27231/27231 [01:38<00:00, 275.27it/s]\n"
     ]
    }
   ],
   "source": [
    "tuples_glove_1 = bias_by_neighbors(gender_bias_glove_1, 'limit_glove_1')   \n",
    "tuples_glove_2 = bias_by_neighbors(gender_bias_glove_2, 'limit_glove_2')  \n",
    "tuples_glove_3 = bias_by_neighbors(gender_bias_glove_3, 'limit_glove_3')  \n",
    "tuples_glove_4 = bias_by_neighbors(gender_bias_glove_4, 'limit_glove_4')  \n",
    "tuples_glove_5 = bias_by_neighbors(gender_bias_glove_5, 'limit_glove_5')  \n",
    "tuples_glove_6 = bias_by_neighbors(gender_bias_glove_6, 'limit_glove_6')  \n",
    "tuples_glove_7 = bias_by_neighbors(gender_bias_glove_7, 'limit_glove_7')\n",
    "tuples_glove_8 = bias_by_neighbors(gender_bias_glove_8, 'limit_glove_8')  \n",
    "tuples_glove_9 = bias_by_neighbors(gender_bias_glove_9, 'limit_glove_9')  \n",
    "tuples_glove_10 = bias_by_neighbors(gender_bias_glove_10, 'limit_glove_10')  \n",
    "\n",
    "tuples_w2v = bias_by_neighbors(gender_bias_w2v, 'limit_w2v')  \n",
    "\n",
    "tuples_fasttext_1 = bias_by_neighbors(gender_bias_fasttext_1, 'limit_fasttext_1')  \n",
    "tuples_fasttext_2 = bias_by_neighbors(gender_bias_fasttext_2, 'limit_fasttext_2')  \n",
    "tuples_fasttext_3 = bias_by_neighbors(gender_bias_fasttext_3, 'limit_fasttext_3')  \n",
    "tuples_fasttext_4 = bias_by_neighbors(gender_bias_fasttext_4, 'limit_fasttext_4')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation between bias-by-projection and bias-by-neighbors\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "def pearson(a,b):\n",
    "   \n",
    "    return scipy.stats.pearsonr(a,b)\n",
    "\n",
    "def compute_corr(tuples, i1, i2):\n",
    "    \n",
    "    a = []\n",
    "    b = []\n",
    "    for t in tuples:\n",
    "        a.append(t[i1])\n",
    "        b.append(t[i2])\n",
    "    assert(len(a)==len(b))    \n",
    "    print(pearson(a,b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8724100178362321, 0.0)\n",
      "(0.8586076557038413, 0.0)\n",
      "(0.8497446347007468, 0.0)\n",
      "(0.8347333503930401, 0.0)\n"
     ]
    }
   ],
   "source": [
    "compute_corr(tuples_glove_1, 1, 2)\n",
    "compute_corr(tuples_glove_2, 1, 2)\n",
    "compute_corr(tuples_glove_3, 1, 2)\n",
    "compute_corr(tuples_glove_4, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe Wiki Gigaword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8442397459468596, 0.0)\n",
      "(0.8119382402476751, 0.0)\n",
      "(0.7730547290513069, 0.0)\n",
      "(0.7444402487664554, 0.0)\n"
     ]
    }
   ],
   "source": [
    "compute_corr(tuples_glove_5, 1, 2)\n",
    "compute_corr(tuples_glove_6, 1, 2)\n",
    "compute_corr(tuples_glove_7, 1, 2)\n",
    "compute_corr(tuples_glove_8, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7697368579080862, 0.0)\n",
      "(0.7702136554401371, 0.0)\n"
     ]
    }
   ],
   "source": [
    "compute_corr(tuples_glove_9, 1, 2)\n",
    "compute_corr(tuples_glove_10, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7406908013529686, 0.0)\n"
     ]
    }
   ],
   "source": [
    "compute_corr(tuples_w2v, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fastText Wiki News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6290830393062714, 0.0)\n",
      "(0.6249758074699922, 0.0)\n"
     ]
    }
   ],
   "source": [
    "compute_corr(tuples_fasttext_1, 1, 2)\n",
    "compute_corr(tuples_fasttext_2, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### fastText Common Crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.6336795662445177, 0.0)\n",
      "(0.6200156403337626, 0.0)\n"
     ]
    }
   ],
   "source": [
    "compute_corr(tuples_fasttext_3, 1, 2)\n",
    "compute_corr(tuples_fasttext_4, 1, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38workshop",
   "language": "python",
   "name": "p38workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
